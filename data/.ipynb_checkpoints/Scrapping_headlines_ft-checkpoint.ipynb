{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping headlines - final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scraping headlines I was using Selenium. For this purpose a chromedriver should be installed. The scraped headlines are stored in a Mongodb document.\n",
    "\n",
    "The archive articles are available only for subscribed users, that is why I not suppose to share niether the headlines, urls, or the articles. However this notebook contains all the necessary codes how to go trough the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and create useful instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "import json\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated random sleeptimes\n",
    "sleep_time = np.random.normal(loc=15, scale=5, size=100).round(3)\n",
    "sleep_time = sleep_time[sleep_time > 5]  # the sleep time average is 15 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Mongodb document to store the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# create a Mongodb document\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "news = myclient['news']\n",
    "\n",
    "# my collections:\n",
    "tesla_news = news['tesla_news']\n",
    "ge_news = news['ge_news']\n",
    "ibm_news = news['ibm_news']\n",
    "goldman_news = news['goldman_news']\n",
    "ford_news = news['ford_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True), 'news'), 'ford_news')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.create_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOP - Class and methods for retrieving headlines and summary texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebNavigator:\n",
    "    '''\n",
    "    This class initiates a chrome webdriver.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome('/Users/flatironschool/chromedriver')\n",
    "        # if there is a modification, the version num helps to track it down\n",
    "        print('Version2')\n",
    "\n",
    "    def login(self, username, password, login_url):\n",
    "        '''This method allows you to login to the targeted website.'''\n",
    "\n",
    "        self.driver.get(login_url)\n",
    "\n",
    "        u = self.driver.find_element_by_id(\"username\")\n",
    "        p = self.driver.find_element_by_id(\"password\")\n",
    "\n",
    "        time.sleep(2.2)\n",
    "        u.send_keys(username)\n",
    "        p.send_keys(password)\n",
    "\n",
    "        time.sleep(1.8)\n",
    "        login_attempt = self.driver.find_element_by_class_name(\"sign-in\")\n",
    "        login_attempt.submit()\n",
    "\n",
    "        time.sleep(np.random.choice(sleep_time, 1).item()/2)  # being polite\n",
    "\n",
    "        elements = self.driver.find_elements_by_tag_name(\n",
    "            'a')  # we must imitate a click\n",
    "        for e in elements:\n",
    "            if e.text in ['SIGN IN', 'Sign In']:\n",
    "                e.click()\n",
    "                break\n",
    "\n",
    "    def navigate(self, url):\n",
    "        self.driver.get(url)\n",
    "\n",
    "    def get_headlines(self, first_page, collection, num_of_pages=1):\n",
    "        '''\n",
    "        This method retrievs and saves the headlines and summeries to a MongoDB collection.\n",
    "        -------------------------------------------------------\n",
    "        Inputs:\n",
    "            first_page: url of the first page of the search result\n",
    "            num_of_pages: the number of pages of the search results\n",
    "            collection: name of the Mongodb collection where to save the scraped headlines\n",
    "        -------------------------------------------------------\n",
    "        Returns:\n",
    "            there is no return, everything is saved directly to Mongodb collection\n",
    "        '''\n",
    "\n",
    "        # create the search urls from the first urls page and the number of pages\n",
    "        if num_of_pages > 1:\n",
    "            search_urls = [first_page] + [first_page + f'&page={n}' \n",
    "                                          for n in range(2, num_of_pages+1)]\n",
    "        else:\n",
    "            search_urls = [first_page]\n",
    "\n",
    "        for url in search_urls:\n",
    "            self.url = url\n",
    "            self.driver.get(url)\n",
    "            # being polite avg 15 sec\n",
    "            time.sleep(np.random.choice(sleep_time, 1).item())\n",
    "            headline_containers = self.driver.find_elements_by_class_name(\n",
    "                'headline-container')\n",
    "\n",
    "            for hc in headline_containers:\n",
    "                # -------------topic---------------\n",
    "                try:\n",
    "                    topic = hc.find_element_by_class_name(\n",
    "                        'category').find_element_by_tag_name('a').text\n",
    "                except:\n",
    "                    topic = 'nan'\n",
    "                # ------------headline-------------\n",
    "                try:\n",
    "                    headline = hc.find_element_by_class_name(\n",
    "                        'headline').find_element_by_tag_name('a').text\n",
    "                except:\n",
    "                    continue\n",
    "                # --------------summary-------------\n",
    "                try:\n",
    "                    summary = hc.find_element_by_class_name(\n",
    "                        'summary-container').find_element_by_tag_name('p').text\n",
    "                except:\n",
    "                    summary = 'nan'\n",
    "                # ---------------date---------------\n",
    "                try:\n",
    "                    date = hc.find_element_by_tag_name('time').text\n",
    "                except:\n",
    "                    continue\n",
    "                # ---------------url-----------------\n",
    "                try:\n",
    "                    url = hc.find_element_by_class_name(\n",
    "                        'headline').find_element_by_tag_name('a').get_attribute('href')\n",
    "                except:\n",
    "                    url = 'nan'\n",
    "\n",
    "                # ---------insert document into collection-------\n",
    "                document = {'headline': headline,\n",
    "                            'summary': summary,\n",
    "                            'topic': topic,\n",
    "                            'date': date,\n",
    "                            'url': url}\n",
    "                collection.insert_one(document)\n",
    "\n",
    "    def close(self):\n",
    "        '''This method closes the window.'''\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the headline/summary text from the search results\n",
    "\n",
    "\n",
    "\n",
    "The advanced search was generated manually on the website, where there was possible to identify the first result page and the number of result pages altogether. Each search result page contains 20 headlines, its topic category, date, short summary and url.\n",
    "\n",
    "The \"search_result\" variable contains the first page of the search result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tesla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version2\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/webpage_user.json\", 'r') as f:\n",
    "    search_result = json.load(f)['tesla_search_results']\n",
    "\n",
    "webpage = WebNavigator()\n",
    "webpage.login(my_username, my_password, login_url)\n",
    "try:\n",
    "    webpage.get_headlines(search_result, tesla_news, num_of_pages=144)\n",
    "except:\n",
    "    print('error: ', webpage.url)\n",
    "webpage.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version2\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/webpage_user.json\", 'r') as f:\n",
    "    search_result = json.load(f)['ge_search_results']\n",
    "\n",
    "webpage = WebNavigator()\n",
    "webpage.login(my_username, my_password, login_url)\n",
    "try:\n",
    "    webpage.get_headlines(search_result, ge_news, num_of_pages=146)\n",
    "except:\n",
    "    print('error: ', webpage.url)\n",
    "webpage.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: where the time is given as \"x hours ago\", time should be changed to the day of scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version2\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/webpage_user.json\", 'r') as f:\n",
    "    search_result = json.load(f)['ibm_search_results']\n",
    "\n",
    "webpage = WebNavigator()\n",
    "webpage.login(my_username, my_password, login_url)\n",
    "try:\n",
    "    webpage.get_headlines(search_result, ibm_news, num_of_pages=101)\n",
    "except:\n",
    "    print('error: ', webpage.url)\n",
    "webpage.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ford:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version2\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/webpage_user.json\", 'r') as f:\n",
    "    search_result = json.load(f)['ford_search_results']\n",
    "\n",
    "webpage = WebNavigator()\n",
    "webpage.login(my_username, my_password, login_url)\n",
    "try:\n",
    "    webpage.get_headlines(search_result, ford_news, num_of_pages=515)\n",
    "except:\n",
    "    print('error: ', webpage.url)\n",
    "webpage.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check the number of documents in Mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_news.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2838"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_news.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldman_news.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1983"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibm_news.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
